{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------\n",
    "# Нейронные сети\n",
    "-------------------------------------------------------------------------------\n",
    "\n",
    "Большой класс задач можно свести к двум основным типам:\n",
    "\n",
    "1. **Задачи регрессии** (предсказание количественного признака объекта на основании других его признаков, например, предсказание роста человека на основании его веса и возраста).\n",
    "2. **Задачи классификации** (предсказание качественного признака объекта на основании других его признаков, например, предсказание пола человека (бинарная классификация) на основании его роста и веса).\n",
    "\n",
    "Для решения этих задач будем рассматривать обучающиеся модели и способы их обучения.\n",
    "\n",
    "Некоторые другие курсы по НН:  \n",
    "- [Stanford course on Computer Vision](http://cs231n.github.io/)\n",
    "- [NN tutorial](http://neuralnetworksanddeeplearning.com/chap1.html)\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "## 1. NumPy\n",
    "\n",
    "Для обработки большого числа данных удобнее и эффективнее использовать сторонние библиотеки, такие как NumPy.\n",
    "\n",
    "### 1.1. Создание массивов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "(2, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "M = np.array([[1,2,3], [4,5,6]])\n",
    "print(M, M.shape, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для массивов в NumPy определены основные операции: `+ - * / // % ** < <= == >= > !=` выполняются поэлементно (если же вторым аргументом бинарной операции указан скаляр, то он преобразуется в массив той же формы, что и первый аргумент)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n",
      "\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "\n",
      "[[5 5 5]\n",
      " [5 5 5]\n",
      " [5 5 5]]\n"
     ]
    }
   ],
   "source": [
    "ls, n, m, value = [1, 2, 3], 3, 3, 5\n",
    "shape = (n, m)\n",
    "M = np.array(ls)                           # создать NumPy массив из списка\n",
    "I = np.eye(n, m, k=0)                      # единичная матрица (k — сдвиг диагонали)\n",
    "O = np.zeros(shape)                        # нулевой массив указанной формы (или np.zeros_like(M))\n",
    "N = np.ones(shape)                         # новый массив указанной формы, заполненный единицами\n",
    "N = np.full(shape, value)                  # новый массив указанной формы, заполненный value\n",
    "print(M, I, O, N, sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Манипуляции с массивами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5 6 7 8 9]\n",
      "\n",
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]\n",
      " [1 2 3]\n",
      " [4 5 6]]\n",
      "\n",
      "[[1 2 3 4 5 6 7 8 9]]\n",
      "\n",
      "[[1 4 7]\n",
      " [2 5 8]\n",
      " [3 6 9]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "M = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "N = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "shape = (1, 9)\n",
    "print(M.flatten(), end='\\n\\n')             # превратить массив в одномерный\n",
    "print(np.concatenate([M, N]), end='\\n\\n')  # конкатенация\n",
    "print(M.reshape(shape), end='\\n\\n')        # изменить форрму массива (массив распрямляется заполняется в новую форму)\n",
    "print(M.T, end='\\n\\n')                     # транспонирование (или смена порядка индексов в случае d > 2 M.transpose(*i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для массивов размерности $d > 2$ при транспонировании индексы ставятся в обратном порядке (по-умолчанию), т.е. $(i[0],..., i[n-1])$ переходит в $(i[n-1],..., i[0])$. Или можно задать параметр-перестановку (кортеж, соответствующей длины, из номеров индексов):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1 7 6]\n",
      "  [2 9 8]]\n",
      "\n",
      " [[7 5 5]\n",
      "  [1 5 7]]]\n",
      "\n",
      "[[[1 2]\n",
      "  [7 9]\n",
      "  [6 8]]\n",
      "\n",
      " [[7 1]\n",
      "  [5 5]\n",
      "  [5 7]]]\n"
     ]
    }
   ],
   "source": [
    "M = np.random.randint(1, 10, size=(12)).reshape((2,2,3))\n",
    "print(M, M.transpose(0,2,1), sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вообще,  \n",
    "(m,1) - двумерный массив (вектор-столбец)  \n",
    "(1,m) - двумерный массив (вектор-строка)  \n",
    "(m,)  - одномерный массив\n",
    "\n",
    "Превращение вектора в двумерный или одномерный объект:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3 4]]\n",
      "\n",
      "[[1 2 3 4]]\n",
      "\n",
      "[[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "\n",
      "[[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "\n",
      "[1 2 3 4] [1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "v = np.array([1, 2, 3, 4])\n",
    "v_row = v[np.newaxis, :]\n",
    "v_col = v[:, np.newaxis]\n",
    "print(v[None], v_row, v[:, None], v_col, sep='\\n\\n', end='\\n\\n')  # 1D to 2D\n",
    "print(v_row[0, :], v_col[:, 0])                                   # 2D to 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Срезы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3] [1 3 4]\n",
      "\n",
      "[False False False  True  True  True False False] [4 5 6]\n",
      "[1 2 3 0 0 0 7 8]\n",
      "\n",
      "[1 2 3]\n",
      "\n",
      "[2 8 4]\n",
      "[[2]\n",
      " [8]\n",
      " [4]]\n"
     ]
    }
   ],
   "source": [
    "M = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n",
    "print(M[0:3], M[[0, 2, 3]], end='\\n\\n')\n",
    "\n",
    "cond = (M > 3) & (M < 7) # & — and, | — or, ~ — not\n",
    "print(cond, M[cond])\n",
    "M[cond] = 0\n",
    "print(M, end='\\n\\n')\n",
    "\n",
    "N = np.copy(M[0:3])\n",
    "print(N, end='\\n\\n')\n",
    "\n",
    "M = np.random.randint(1, 10, size=(3, 3))\n",
    "print(M[:, 1], M[:, 1:2], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Основные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6 5 2]\n",
      " [8 4 8]\n",
      " [9 1 3]]\n",
      "1 9 5.111111111111111 2.6851213274654606\n",
      "7 6 False\n",
      "46 414720 [ 6 11 13 21 25 33 42 43 46] [     6     30     60    480   1920  15360 138240 138240 414720]\n"
     ]
    }
   ],
   "source": [
    "M = np.random.randint(1, 10, size=(3, 3))\n",
    "print(M)\n",
    "\n",
    "# мин, макс, среднее, стандартное отклонение вдоль указанных индексов (или всего массива, если не указаны):\n",
    "print(M.min(axis=None), M.max(axis=None), M.mean(axis=None), M.std(axis=None))\n",
    "\n",
    "# индексы мин и макс элементов вдоль указанных индексов (или всего массива), равенство массивов:\n",
    "print(M.argmin(axis=None), M.argmax(axis=None), np.array_equal(M, M*2))\n",
    "\n",
    "# сумма, пр-ие, частичная сумма, частичное пр-ие элементов вдоль указанных индексов (или всего массива):\n",
    "# для (a1,...,an) вектор частичных сумм — это (a1, a1+a2,..., a1+...+an))\n",
    "print(M.sum(axis=None), M.prod(axis=None), M.cumsum(axis=None), M.cumprod(axis=None))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Линейная алгебра"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 0. 0.]\n",
      " [0. 2. 0.]\n",
      " [0. 0. 2.]]\n",
      "\n",
      "3.4641016151377544\n",
      "\n",
      "[[4. 0. 0.]\n",
      " [0. 4. 0.]\n",
      " [0. 0. 4.]]\n",
      "\n",
      "[[0.5 0.  0. ]\n",
      " [0.  0.5 0. ]\n",
      " [0.  0.  0.5]]\n",
      "\n",
      "[[0.5 0.  0. ]\n",
      " [0.  0.5 0. ]\n",
      " [0.  0.  0.5]]\n"
     ]
    }
   ],
   "source": [
    "M = np.eye(3, 3) * 2\n",
    "N = np.eye(3, 3)\n",
    "print(M @ N, end='\\n\\n')  # или np.dot(M, N)\n",
    "print(np.linalg.norm(M, ord=None), np.linalg.matrix_power(M, n=2), np.linalg.inv(M), np.linalg.pinv(M), sep='\\n\\n')\n",
    "# по умолчанию норма Фробениуса для матриц и L2-норма для векторов; inv и pinv — обратная и псевдообратная матрицы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. Чтение данных\n",
    "\n",
    "Данные можно читать из файла функцией **loadtxt**:\n",
    "\n",
    "```python\n",
    "data = np.loadtxt(\"demo_data.csv\", usecols=(0,1,4), skiprows=1, delimiter=\",\", \n",
    "                      dtype={'names': ('date', 'open', 'close'),\n",
    "                             'formats': ('datetime64[D]', 'f4', 'f4')})\n",
    "```\n",
    "\n",
    "Ввод данных вручную:\n",
    "```python\n",
    "x_shape = tuple(map(int, input().split()))\n",
    "X = np.fromiter(map(int, input().split()), np.int).reshape(x_shape)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------\n",
    "## 2. Линейная регрессия\n",
    "\n",
    "Типы переменных:  \n",
    "1. Количественные (порядок и метрика)\n",
    "2. Порядковые (порядок без метрики)\n",
    "3. Категориальные (ни порядка, ни метрики)\n",
    "\n",
    "### 2.1. Линейная регрессия\n",
    "\n",
    "$$\n",
    "\\mathbf{Y} = \\mathbf{X} \\, \\beta + \\mathbf{\\varepsilon} \\; \\left(\\text{или} \\; \\mathbf{Y} = \\mathbf{X} \\, \\beta + \\beta_0 + \\mathbf{\\varepsilon} \\right).\n",
    "$$\n",
    "\n",
    "- набор наблюдений $X$ (таблица n:m, или n:m+1 с первым столбцом из единиц)\n",
    "- целевая переменная $Y$ (вектор n:1)\n",
    "- линейная связь $\\beta$ (вектор m:1, или m+1:1 вместе со свободным членом $\\beta_0$)\n",
    "- нормальное распределения ошибок $\\mathbf{\\varepsilon}$ (вектор n:1)\n",
    "\n",
    "|     | x0   | x1 | ... |  xm |  y  |\n",
    "|:---:|:----:|:--:|:---:|:---:|:---:|\n",
    "|  1  | 1    |    |     |     |     |\n",
    "|  2  | 1    |    |     |     |     |\n",
    "| ... | ...  |    |     |     |     |\n",
    "|  n  | 1    |    |     |     |     |\n",
    "\n",
    "### 2.2. Метод наименьших квадратов\n",
    "\n",
    "Будем искать параметры модели методом наименьших квадратов (см. конспект по численным методам). Выбор этого метода обосновывается теоретически из метода максимального правдоподобия для нормально распределённых ошибок. Практически же понять выбор квадратов вместо модулей можно на примере горизонтальной линии: минимизируя модули можно провести прямую где угодно между точками ошибок, минимизация же квадратов заставит провести прямую ровно посередине.\n",
    "\n",
    "Запишем уравнение регрессии в матричном виде для вектора предсказаний целевой переменной $\\mathbf{\\hat{Y}}$, используя вектор оценок коэффициентов $\\hat{\\beta}$ и вектор ошибок предсказаний $\\hat{\\varepsilon}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\hat{Y}} = \\mathbf{X} \\, \\hat{\\beta}, \\\\\n",
    "\\mathbf{\\hat{\\varepsilon}} = \\mathbf{Y} - \\mathbf{\\hat{Y}}.\n",
    "$$\n",
    "\n",
    "Тогда метод наименьших квадратов заключается в минимизации $\\mathbf{\\hat{\\varepsilon}}^T \\cdot \\mathbf{\\hat{\\varepsilon} = \\sum_{i = 1}^n \\left(y^{(i)} - \\hat{y}^{(i)}\\right)^2} \\,$ для всего набора наблюдений (т.е. минимизации $\\left\\Vert \\mathbf{\\hat{\\varepsilon}} \\right\\Vert^2 = \\langle\\hat{\\varepsilon}, \\hat{\\varepsilon} \\rangle $). В результате оптимальные коэффициенты $\\beta$ можно найти либо используя методы анализа (нахождение минимума функции), либо средствами линейной алгебры (с помощью псевдообратной матрицы).\n",
    "\n",
    "Например, для линейного приближения целевой переменной $y$ по одной переменной наблюдений $x$ (т.е. $\\hat{y}^{(i)} = x^{(i)} \\, \\hat{\\beta}_1 + \\hat{\\beta}_0$) можно получить:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_1 = \\frac{\\bar{x y} - \\bar{x} \\bar{y}}{\\bar{x^2} - \\bar{x}^2}, \\quad \\hat{\\beta}_0 = \\bar{y} - \\beta_1 \\bar{x}.\n",
    "$$\n",
    "\n",
    "### 2.3. Общиий вид решения\n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = \\mathbf{X}^+ \\mathbf{Y} = \\left(\\mathbf{X}^T X \\right)^{-1} \\mathbf{X}^T \\mathbf{Y},\n",
    "$$\n",
    "\n",
    "где $\\mathbf{X}^+$ — [псевдообратная матрица](https://ru.wikipedia.org/wiki/%D0%9F%D1%81%D0%B5%D0%B2%D0%B4%D0%BE%D0%BE%D0%B1%D1%80%D0%B0%D1%82%D0%BD%D0%B0%D1%8F_%D0%BC%D0%B0%D1%82%D1%80%D0%B8%D1%86%D0%B0) (вводится для обобщения понятия обратимости для неквадратных или вырожденных матриц).\n",
    "\n",
    "Кстати, это позволяет ввести оператор $H = \\mathbf{X} \\mathbf{X}^+$; он является самосопряжённым ($H^T = H$) и идемпотентным ($H^n = H \\; \\forall n \\in \\mathbb{N}$). Этот оператор переводит $Y$ в $\\hat{Y}$, действительно $H \\mathbf{Y} = \\mathbf{X} \\mathbf{X}^+ \\mathbf{Y} = \\mathbf{X} \\hat{\\beta} = \\mathbf{\\hat{Y}}$.\n",
    "\n",
    "**Пример**:\n",
    "\n",
    "Найти оптимальные коэффициенты для построения линейной регрессии по данным таблицы. Задача — предсказание первой переменной (первого столбца данных) по всем остальным. Напечатать результат начиная с $\\beta_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.655804285064761 -0.21639550236913585 0.07373059817548458 4.412450576912802 -25.468448784098424 7.143201550746399 -1.3010876776489941\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open(\"data/boston_houses.csv\") as f:\n",
    "    #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------\n",
    "## 3. Нейроны\n",
    "\n",
    "### 3.1. Нейропластичность\n",
    "\n",
    "**Нейропластичность** — способность мозга существенно меняться в течение жизни и адаптироваться под изменяющиеся условия среды.\n",
    "\n",
    "**Примеры**:\n",
    "\n",
    "1. Эксперименты по перепроводке путей следования информации в головном мозге (rewiring).\n",
    "\n",
    "    Путь зрительной информации: глаза → латеральное коленчатое тело ([lateral geniculate nucleus](https://en.wikipedia.org/wiki/Lateral_geniculate_nucleus)) → первичная зрительная кора.  \n",
    "    Путь звуковой информации: уши → медиальное коленчатое тело ([medial geniculate nucleus](https://en.wikipedia.org/wiki/Medial_geniculate_nucleus)) → аудиальная кора.\n",
    "\n",
    "    - эксперимент с хорьками, которым перенаправили пути зрительной информации к медиальному коленчатому телу, демонстрирует явление нейропластичности мозга.\n",
    "    - эксперимент с [очками](https://en.wikipedia.org/wiki/Upside_down_goggles), переворачивающими изображение (со временем мозг адаптируется).\n",
    "\n",
    "2. [Сенсорное замещение](https://en.wikipedia.org/wiki/Sensory_substitution).\n",
    "\n",
    "    **Сенсорное замещение** — использование одного анализатора для сбора информации, обычно получаемой через другой сенсорный орган.\n",
    "\n",
    "    - эксперимент, позволяющий видеть спиной: на спинке кресла устанавливается решетка из вибростимуляторов и вместе с вибрациями показывают изображения, со временем участники эксперимента приучались видеть спиной и узнавать лица известных людей.\n",
    "    - аналогичные эксперименты, но с языком — [Brain port](https://en.wikipedia.org/wiki/Brainport) для слепых людей. При этом активны зоны мозга, отвечающие именно за зрение.\n",
    "    - [кохлеарный имплантат](https://en.wikipedia.org/wiki/Cochlear_implant) для слабослышащих. \n",
    "\n",
    "Именно свойство нейропластичности наталкивает на мысль моделирования нейронных сетей (или некоторых принципов их работы) для реализации процесса обучения и адаптации.\n",
    "\n",
    "### 3.2. Нейроны\n",
    "\n",
    "**Нейрон** — функциональная единица нервной системы.\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"data/neuron_bio.png\" width=\"500\" title=\"Нейрон\"></p>\n",
    "<!-- ![Нейрон](\"data/neuron_bio.png\") -->\n",
    "\n",
    "Каждый нейрон образует 5-200k соединений с другими нейронами, при этом связь между ними также характеризуется количеством и типом синаптических соединений, какие используются нейромедиаторы, где конкретно на теле нейрона располагаются соединения и т.д. ([здесь подробней](https://habr.com/en/post/214109/)). Именно такое большое количество параметров обеспечивает биологическим нейронным сетям высокий уровень гибкости, однако для их моделирования логично игнорировать большую часть этой сложности.\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"data/Artificial_Neuron_Scheme.png\" width=\"400\" title=\"Схема нейрона\"></p>\n",
    "\n",
    "Схема искусственного нейрона:\n",
    "1. входы (с весами)\n",
    "2. сумматорная функция\n",
    "3. активационная функция\n",
    "4. выходы\n",
    "\n",
    "Обучение — изменение весов нейронов сети на основе новых данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------\n",
    "## 4. Перцептрон\n",
    "\n",
    "1960 — простая модель искусственного нейрона (F.Rosenblatt)  \n",
    "1969 — показали ограничения перцептрона (M.Minsky, S.Papert)  \n",
    "1974-1980 — AI winter  \n",
    "2010+ — применяются для решения задач с большим количеством предикторов (переменных, на основании которых делается предсказание о целевой переменной).\n",
    "\n",
    "### 4.1. Модель перцептрона\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}, \\mathbf{w}, b) = \\begin{cases} 1, & \\displaystyle\\sum_{i=1}^n w_i x_i + b \\gt 0, \\\\ 0, & \\displaystyle\\sum_{i=1}^n w_i x_i + b \\leqslant 0. \\end{cases}\n",
    "$$\n",
    "\n",
    "- вектор входных активаций $\\mathbf{x}$\n",
    "- вектор весов $\\mathbf{w}$ (weights)\n",
    "- смещение $b$ (bias)\n",
    "- выходная активация перцептрона $f(\\mathbf{x}, \\mathbf{w}, b)$\n",
    "\n",
    "Если принять $x_0 = 1$, а $w_0 = b$, то модель перепишется в виде:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}, \\mathbf{w}) = \\begin{cases} 1, & \\langle \\mathbf{x}, \\mathbf{w} \\rangle \\gt 0, \\\\ 0, & \\langle \\mathbf{x}, \\mathbf{w} \\rangle \\leqslant 0. \\end{cases}\n",
    "$$\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"data/perceptron_scheme.png\" width=\"500\" title=\"Перцептрон\"></p>\n",
    "\n",
    "**Замечание**:\n",
    "\n",
    "Перцептроном можно также моделировать логические функции (т.е. перцептрон является универсальным вычислительным элементом). Например, если входной слой состоит из трёх бинарных элементов (при этом $x_0 = 1$) с весами $w_0 = 2, \\, w_1 = w_2 = -1$, то перцептрон будет вычислять функцию NAND (not and). При этом она является базисом для булевых функций двух переменных, то есть, пользуясь только этим логическим элементом, можно выразить любую другую логическую функцию.\n",
    "\n",
    "### 4.2. Обучение перцептрона\n",
    "\n",
    "Процесс обучения:\n",
    "\n",
    "1. Задать случайно веса.\n",
    "2. Давать примеры на вход, а потом показывать правильные для них ответы.\n",
    "3. Изменять веса на основе результатов.\n",
    "\n",
    "**Алгоритм обучения**:\n",
    "\n",
    "```python\n",
    "perfect = False\n",
    "while not perfect:\n",
    "    perfect = True                       # X        — конкретный пример из данных (вектор входных активаций)\n",
    "    for X in data:                       # Y_hat(X) — результат перцептрона на примере X\n",
    "        if Y_hat(X) != Y(X):             # Y(X)     — истинный результат на примере X\n",
    "            perfect = False              # w        — вектор весов\n",
    "            if Y_hat(X) == 0:\n",
    "                w += X\n",
    "            else:\n",
    "                w -= X\n",
    "```\n",
    "\n",
    "Т.е. в результате обучения веса перцептрона меняются на $\\Delta w_i = (y - \\hat{y}) \\, x_i$ (т.к. $y$ может быть только 0 или 1). А именно, если неправильно определился позитивный пример ($y = 1, \\hat{y} = 0$), то к $\\mathbf{w}$ прибавляем $\\mathbf{x}$, если же неправильно определился негативный пример ($y = 0, \\hat{y} = 1$), то от $\\mathbf{w}$ отнимаем $\\mathbf{x}$. Если же предсказание верное, то вектор весов не трогаем. Почему этот алгоритм работает можно понять геометрически.\n",
    "\n",
    "### 4.3. Геометрическая интерпретация алгоритма\n",
    "\n",
    "Конкретный пример $\\mathbf{x}$, как и набор весов $\\mathbf{w}$ — это n-мерные векторы. Значение активационной функции перцептрона зависит от того, какой угол между этими векторами (острый или тупой). Кроме того, каждый вектор в n-мерном пространстве однозначно определяет гиперплоскость (и наоборот), т.е. значение активационной функции также можно определить тем, в каком из полупространств, полученных делением n-мерного пространства соответствующей $\\mathbf{x}$ гиперплоскостью, лежит вектор весов $\\mathbf{w}$.\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"data/geometric_perc.png\" width=\"500\" title=\"Перцептрон\"></p>\n",
    "\n",
    "Если же примеров несколько, то перцептрон будет давать правильный ответ для каждого из них, только если вектор весов $\\mathbf{w}$ составляет нужный угол с каждым из $\\mathbf{x}$ (или если вектор $\\mathbf{w}$ лежит в нужной области пересечений полупространств, получаемых делением гиперплоскостями примеров $\\mathbf{x}$).\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"data/perceptron_geometry_1.png\" width=\"750\" title=\"алгоритм 1\"></p>\n",
    "\n",
    "Чтобы при выполнении алгоритма обучения при изменении вектора весов $\\mathbf{w}$ не проскочить область подходящих весов её уменьшают следующим образом:\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"data/perceptron_geometry_2.png\" width=\"400\" title=\"алгоритм 2\"></p>\n",
    "\n",
    "**Замечание**:\n",
    "\n",
    "Вообще, если решение существует, то при выполнении алгоритма обучения вектор весов $\\mathbf{w}$ к нему сойдётся. Однако, решение для однонейронного перцептрона может не найтись даже в очень простой ситуации, например, при реализации операции XOR (хотя это ограничение можно обойти добавлением дополнительного слоя между входами и активационной функцией).\n",
    "\n",
    "Геометрически же это выражается тем, что **границу (поверхность) решения** $w_1 x_1 + w_2 x_2 + b = 0$ для данной задачи нельзя провести так, чтобы все примеры одного класса лежали по одну сторону от неё, а все примеры второго класса — по другую сторону (т.е. набор данных не является линейно разделимым).\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"data/XOR.png\" width=\"300\" title=\"XOR\"></p>\n",
    "\n",
    "### 4.4. Типы нейронов\n",
    "\n",
    "1. **Линейный**\n",
    "$$\n",
    "\\begin{multline}\n",
    "f(\\mathbf{x}, \\mathbf{w}) = \\mathbf{x} \\cdot \\mathbf{w} \\,.\n",
    "\\end{multline}\n",
    "$$\n",
    "2. **Логистический**\n",
    "$$\n",
    "\\begin{multline}\n",
    "f(\\mathbf{x}, \\mathbf{w}) = \\sigma (\\mathbf{x} \\cdot \\mathbf{w}) = \\dfrac{1}{1 + \\exp (- \\, \\mathbf{x} \\cdot \\mathbf{w})} \\,.\n",
    "\\end{multline}\n",
    "$$\n",
    "3. **Гиперболический тангенс**\n",
    "$$\n",
    "\\begin{multline}\n",
    "f(\\mathbf{x}, \\mathbf{w}) = \\tanh (\\mathbf{x} \\cdot \\mathbf{w}) = \\dfrac{\\exp(\\mathbf{x} \\cdot \\mathbf{w}) - \\exp(- \\, \\mathbf{x} \\cdot \\mathbf{w})}{\\exp(\\mathbf{x} \\cdot \\mathbf{w}) + \\exp(- \\, \\mathbf{x} \\cdot \\mathbf{w})} \\,.\n",
    "\\end{multline}\n",
    "$$\n",
    "<p style=\"text-align:center;\"><img src=\"data/sigmoid_tanh.png\" width=\"500\" title=\"plots\"></p>  \n",
    "\n",
    "    - $\\sigma(-x) = 1 - \\sigma(x) \\,,$\n",
    "    \n",
    "    - $\\sigma'(x) = \\sigma(x) (1 - \\sigma(x)) \\,,$\n",
    "    \n",
    "    - $\\sigma(2 x) = \\dfrac{1 + \\tanh(x)}{2} \\,,$\n",
    "    \n",
    "    - $\\tanh(-x) = -\\tanh(x) \\,,$\n",
    "    \n",
    "    - $\\tanh'(x) = \\dfrac{1}{\\cosh^2(x)} \\,.$\n",
    "\n",
    "4. **ReLU** (rectified linear unit или [Ramp function](https://en.wikipedia.org/wiki/Ramp_function))\n",
    "$$\n",
    "\\begin{multline}\n",
    "f(\\mathbf{x}, \\mathbf{w}) = \\max(\\mathbf{x} \\cdot \\mathbf{w}, 0) \\,.\n",
    "\\end{multline}\n",
    "$$\n",
    "5. **Softplus** (непрерывная аппроксимация ReLU)\n",
    "$$\n",
    "\\begin{multline}\n",
    "f(\\mathbf{x}, \\mathbf{w}) = \\ln(1 + \\exp(\\mathbf{x} \\cdot \\mathbf{w})) \\,.\n",
    "\\end{multline}\n",
    "$$\n",
    "<p style=\"text-align:center;\"><img src=\"data/ReLU_plot.png\" width=\"600\" title=\"plots\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------\n",
    "## 5. Градиентный спуск\n",
    "\n",
    "### 5.1. Функция потерь\n",
    "\n",
    "**Функция потерь** — мера успешности работы нейрона на представленных данных.\n",
    "\n",
    "При обучении нейрона, критерий качества его работы определяется **функцией потерь**, которая сравнивает полученные активационной функцией значения с истинными значениями целевой переменной. Функция потерь определяется параметрами модели (т.е вектором весов $\\mathbf{w}$), так как данные считаем фиксированными. Например, можно использовать **квадратичную целевую функцию**:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{w}) = \\frac{1}{2 n} \\sum_{i=1}^n \\left(\\hat y^{(i)} - y^{(i)}\\right)^2 \\,, \\text{где } (i) \\text{ — номер примера} \\,.\n",
    "$$\n",
    "\n",
    "**Loss (cost) function** — функция потерь на одном конкретном примере (для квадратичной целевой функции это $\\dfrac{1}{2 n} \\left(\\hat y^{(i)} - y^{(i)}\\right)^2$).\n",
    "\n",
    "|     |  1 |  2 | ... |  n |\n",
    "|:---:|:--:|:--:|:---:|:--:|\n",
    "|  x0 |  1 |  1 | ... |  1 |\n",
    "|  x1 |    |    |     |    |\n",
    "| ... |    |    |     |    |\n",
    "|  xm |    |    |     |    |\n",
    "|  y  | y1 | y2 | ... | yn |\n",
    "\n",
    "### 5.2. Метод градиентного спуска\n",
    "\n",
    "Для нахождения глобального минимума функции потерь (т.е. чтобы нейрон работал максимально точно) можно использовать разные численные методы, например, градиентный спуск.\n",
    "\n",
    "$$\n",
    "\\mathbf{w'} = \\mathbf{w} - \\alpha \\, \\nabla J, \\quad \\text{где } \\nabla J = \\begin{pmatrix} \\dfrac{\\partial J}{\\partial w_0} \\\\ \\ldots \\\\ \\dfrac{\\partial J}{\\partial w_m} \\end{pmatrix} \\,, \\quad \\text{а } \\alpha \\in \\mathbb{R} \\text{ — скорость обучения (learning rate).} \n",
    "$$\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"data/grad_descent.png\" width=\"600\" title=\"plots\"></p>\n",
    "\n",
    "- выполняем алгоритм до тех пор, пока веса почти не перестанут меняться или же пока не достигнуто предельное количество шагов.\n",
    "- алгоритм может сойтись к локальному минимуму или к плато и остановиться.\n",
    "- также необходимо удачно выбирать шаг (learning rate): не слишком маленький (долгое время выполнения и можно застрять в локальных минимумах) и не слишком большой (неверный результат).\n",
    "- для экономии ресурсов градиент можно считать по неполным данным (стохастический градиентный спуск)\n",
    "\n",
    "### 5.3. Примеры\n",
    "\n",
    "1. **Задача регрессии для линейного нейрона**.\n",
    "\n",
    "    Компоненты градиента функции потерь для случая $\\mathbf{\\hat{y}^{(i)}} = \\mathbf{w} \\cdot \\mathbf{x^{(i)}}$ получаются равны:\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial J}{\\partial w_j} = \\dfrac{\\partial}{\\partial w_j} \\left( \\dfrac{1}{2 n} \\sum_{i=1}^n \\left(\\hat y^{(i)} - y^{(i)}\\right)^2 \\right) = \\sum_{i = 1}^{n} \\dfrac{\\partial J}{\\partial \\hat{y}^{(i)}} \\dfrac{\\partial \\hat{y}^{(i)}}{\\partial w_j} = \\dfrac{1}{n} \\sum_{i = 1}^{n} \\left(\\hat y^{(i)} - y^{(i)}\\right) \\, x_j^{(i)}.\n",
    "$$  \n",
    "\n",
    "    Тогда градиент запишется в виде:\n",
    "\n",
    "$$\n",
    "\\nabla J = \\dfrac{1}{n} \\sum_{i = 1}^{n} \\left(\\hat y^{(i)} - y^{(i)}\\right) \\begin{pmatrix} x_0^{(i)} \\\\ x_1^{(i)} \\\\ \\ldots \\\\ x_m^{(i)} \\end{pmatrix} = \\dfrac{1}{n} \\, \\begin{pmatrix} (\\mathbf{\\hat{y}} - \\mathbf{y}) \\cdot \\mathbf{x}_0 \\\\ (\\mathbf{\\hat{y}} - \\mathbf{y}) \\cdot \\mathbf{x}_1 \\\\ \\cdots \\\\ (\\mathbf{\\hat{y}} - \\mathbf{y}) \\cdot \\mathbf{x}_m \\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "2. **Задача классификация для сигмоидального нейрона.**\n",
    "\n",
    "    Вообще, для задач классификации квадратичная целевая функция может быть не удобна, но для простоты будем всё-равно её использовать. Тогда для нашего случая $\\mathbf{\\hat{y}^{(i)}} = \\sigma (\\mathbf{w} \\cdot \\mathbf{x^{(i)}})$ компоненты градиента будут равны:\n",
    "    \n",
    "$$\n",
    "\\begin{multline}\n",
    "\\dfrac{\\partial J}{\\partial w_j} = \\dfrac{\\partial}{\\partial w_j} \\left( \\dfrac{1}{2 n} \\sum_{i=1}^n \\left(\\hat y^{(i)} - y^{(i)}\\right)^2 \\right) = \\sum_{i = 1}^{n} \\dfrac{\\partial J}{\\partial \\hat{y}^{(i)}} \\dfrac{\\partial \\hat{y}^{(i)}}{\\partial (\\mathbf{w} \\cdot \\mathbf{x^{(i)}})} \\dfrac{\\partial (\\mathbf{w} \\cdot \\mathbf{x^{(i)}})}{\\partial w_j} = \\frac{1}{n} \\sum_{i=1}^n \\left( \\sigma (\\mathbf{w} \\cdot \\mathbf{x^{(i)}}) - y^{(i)}\\right) \\sigma(\\mathbf{w} \\cdot \\mathbf{x^{(i)}}) \\left(1 - \\sigma(\\mathbf{w} \\cdot \\mathbf{x^{(i)}})\\right) x^{(i)}_j .\n",
    "\\end{multline}\n",
    "$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------\n",
    "## 6. Метод обратного распространения ошибки\n",
    "\n",
    "### 6.1. Многослойный перцептрон\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"data/multi_perceptron_1.png\" width=\"550\" title=\"multi_perceptron\"></p>\n",
    "\n",
    "1. **Входной слой**\n",
    "\n",
    "    $X^{(i)}_{j}$ — матрица (m, n) входных данных (где $x^{(i)}$ — вектор (m, 1) входных данных для i-ого примера).\n",
    "\n",
    "2. **Скрытые слои**\n",
    "\n",
    "    $w^{l}_{j k}$ — матрица весов для нейронов $l$-ого слоя (вес между $j$-ым нейроном $l$-ого слоя и $k$-ым нейроном ($l-1$)-ого слоя).  \n",
    "    $z^l_j$ — сумматорная функция для нейронов $l$-ого слоя.  \n",
    "    $a^l_j$ — активационная функция для нейронов $l$-ого слоя.\n",
    "    \n",
    "    Смещения же в каждом слое можно задать с помощью добавления дополнительного нейрона с постоянным единичным значением и весами $b^l_j$. Другой способ — считать смещения принадлежащими каждому нейрону (будем пользоваться этим вариантом). Тогда, $b^l$ — вектор смещений для нейронов $l$-ого слоя (не считая первый слой).\n",
    "    $$\n",
    "    a^{l}_j = f\\left( z^l_j \\right) = f \\left( \\sum_k w^{l}_{jk} a^{l-1}_k + b^l_j \\right), \\quad \\text{или } \\, \\mathbf{a^l} = f \\left( W^l \\mathbf{a^{l-1}} + \\mathbf{b^l} \\right) \\,.\n",
    "    $$\n",
    "\n",
    "3. **Выходные слои**\n",
    "\n",
    "    - можно выбирать разные активационные функции для разных слоёв.  \n",
    "    - для задач классификации можно представлять ответ как единичный вектор, совпадающий по направлению с одной из осей, где оси — это классы. Реализовать это можно добавлением нейронов в выходной слой (количество нейронов равно количеству классов в задаче), тогда в результате получим вектор, проецируя который на одну из осей получим ответ.\n",
    "    - тогда квадратичная функция потерь запишется как\n",
    "    \n",
    "    $$\n",
    "    J = \\frac{1}{2n} \\sum_{i=1}^{n}\\left\\vert y^{(i)} - \\hat{y}^{(i)}\\right\\vert^2, \\, \\text{где } y^{(i)} \\text{ — вектор правильного ответа для i-ого примера, а } \\hat{y}^{(i)} \\text{ — вектор полученного ответа.}\n",
    "    $$\n",
    "\n",
    "С помощью многослойного перцептрона можно находить нелинейные границы решения в задачах классификации (вообще, нейронные сети — это универсальный аппроксиматор, т.е. нейронной сетью можно с любой наперёд заданной точностью приблизить произвольную непрерывную функцию). Тогда нереализуемая для однонейронного перцептрона функция **XOR** с лёгкостью моделируется многослойным:\n",
    "\n",
    "$$a \\, \\mathrm{XOR} \\, b = \\left(\\bar{a} \\wedge b \\right) \\vee \\left(a \\wedge \\bar{b} \\right)\\,.$$\n",
    "\n",
    "Подобрать необходимые веса для функции **XOR** можно записав неравенства для них, или же просто смоделировать каждый нейрон по отдельности:\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"data/multi_XOR.jpg\" width=\"400\" title=\"XOR\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Метод обратного распространения ошибки\n",
    "\n",
    "Идея — применить градиентный спуск для обучения многослойного перцептрона, т.е. найти веса перцептрона, минимизирующие функцию потерь. Для решения этой задачи для каждого нейрона вводят вспомогательную величину\n",
    "\n",
    "$$\n",
    "\\delta_j^l \\stackrel{def}{=} \\dfrac{\\partial J}{\\partial z_j^l} = \\dfrac{\\partial J}{\\partial a_j^l} \\, \\dfrac{\\partial a_j^l}{\\partial z_j^l}\n",
    "$$\n",
    "\n",
    "— **ошибку** $j$-ого нейрона из слоя $l$, или в векторном виде для всего $l$-ого слоя  это запишется как $\n",
    "\\delta^l = \\nabla_a J \\odot f’(z^l)$, где $\\odot$ — поэлементное умножение векторов. Через $\\delta_j^l$ позже можно будет выразить $\\frac{\\partial J}{\\partial w_{i j}^l}$ и $\\frac{\\partial J}{\\partial b_j^l}$, т.е. компоненты градиента $J$.\n",
    "\n",
    "<!-- <img src=\"data/multi_perceptron_1.png\" width=\"550\" title=\"plots\"> -->\n",
    "\n",
    "Изменение выходной активации в $j$-ом нейроне слоя $l$ влечёт за собой изменение сумматорных функций в нейронах слоя $l+1$. Т.е. если $a_j^l \\rightarrow a_j^l + da_j^l$ , то\n",
    "\n",
    "$$\n",
    "z_i^{l+1} = \\sum_j w_{i j}^{l+1} \\, a_j^l \\, + b_j^{l+1} \\, \\longrightarrow \\sum_j w_{i j}^{l+1} \\, \\left(a_j^l + da_j^l \\right) \\, + b_j^{l+1} = \\sum_j w_{i j}^{l+1} \\, a_j^l \\, + b_j^{l+1} + \\sum_j w_{i j}^{l+1} \\, da_j^l = z_i^{l+1} + dz_i^{l+1} \\,.\n",
    "$$\n",
    "\n",
    "Т.е. $\\mathbf{da}^l$ — вектор изменений активационных функций в нейронах слоя $l$, а $\\mathbf{dz}^{l+1} = W^{l+1} \\, \\mathbf{da}^l$ — вектор соответствующих им изменений сумматорных функций в слое $l+1$. Тогда для дифференциала функции потерь получим\n",
    "\n",
    "$$\n",
    "\\begin{multline}\n",
    "dJ \\left( z_1^{l+1}, \\dots , z_{n_{l+1}}^{l+1} \\right) = \\nabla_{z^{l+1}} \\mathbf{J} \\cdot \\mathbf{dz}^{l+1} = \\sum_i \\dfrac{\\partial J}{\\partial z_i^{l+1}} \\, dz_i^{l+1} = \\delta^{l+1} \\cdot \\left( W^{l+1} \\, \\mathbf{da}^l \\right) = \\sum_{i j} \\delta_i^{l+1} w_{i j}^{l+1} da_j^l \\,.\n",
    "\\end{multline}\n",
    "$$\n",
    "\n",
    "Отсюда можно понять, что частные производные функции потерь по активационным функциям предыдущего слоя выразятся как\n",
    "\n",
    "$$\n",
    "\\begin{gather}\n",
    "\\dfrac{\\partial J}{\\partial a_j^l} = \\sum_i \\delta_i^{l+1} w_{i j}^{l+1} \\,, \\\\\n",
    "\\dfrac{\\partial J}{\\partial z_j^l} = \\dfrac{\\partial a_j^l}{\\partial z_j^l} \\sum_i \\delta_i^{l+1} w_{i j}^{l+1} \\,, \\\\\n",
    "\\delta^l = f'\\left( z^l \\right) \\, \\odot \\, \\left(W^{l+1}\\right)^T \\, \\delta^{l+1}\\,.\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "Т.о. получили связь между значениями ошибок в нейронах соседних слоёв. Используя выражение $z^l_j = \\sum_k w^{l}_{jk} a^{l-1}_k + b^l_j$ и правило дифференцирования сложной функции, осталось выразить компоненты градиента $J$ через ошибки:\n",
    "\n",
    "$$\n",
    "\\begin{gather}\n",
    "\\dfrac{\\partial J}{\\partial w_{i j}^l} = \\dfrac{\\partial J}{\\partial z_j^l} \\, \\dfrac{\\partial z_j^l}{\\partial w_{i j}^l} = \\delta^l_i \\, a^{l-1}_j \\,, \\\\\n",
    "\\dfrac{\\partial J}{\\partial b_j^l} = \\dfrac{\\partial J}{\\partial z_j^l} \\, \\dfrac{\\partial z_j^l}{\\partial b_j^l} = \\delta^l_j \\,.\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "**Алгоритм**:\n",
    "0. Выбрать набор примеров (batch) из всей выборки данных.\n",
    "1. Посчитать активации нейронов сети для всех примеров (**forward propagation**). \n",
    "2. Посчитать ошибки нейронов выходного слоя для всех примеров:\n",
    "\n",
    "    $$\n",
    "    \\delta^L = \\nabla_a J \\odot f'(z^L) \\,.\n",
    "    $$\n",
    "    \n",
    "3. Посчитать ошибки нейронов для всех слоёв сети (**backpropagation**):\n",
    "    \n",
    "    $$\n",
    "    \\delta^l = \\left(\\left(W^{l+1}\\right)^T\\delta^{l+1}\\right) \\odot f'(z^l) \\,.\n",
    "    $$\n",
    "    \n",
    "4. Найти компоненты градиента:\n",
    "\n",
    "    $$\n",
    "    \\frac{\\partial J}{\\partial w^l_{jk}} = a^{l-1}_{k} \\delta_j^l, \\quad \\frac{\\partial J}{\\partial b^l_j} = \\delta^l_j \\,.\n",
    "    $$\n",
    "    \n",
    "5. Обновить веса и проверить критерии остановки алгоритма.\n",
    "\n",
    "**Алгоритм легко сломать если**:    \n",
    "1. поставить одинаковые веса при инициализации сети (одинаковые нейроны)\n",
    "2. выбрать слишком большие веса или значения входных активации (ступор сети)\n",
    "3. выбрать слишком маленькие веса или значения входных активации (медленное обучение)\n",
    "\n",
    "Дополнительно:  \n",
    "- https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\n",
    "- http://neuralnetworksanddeeplearning.com/chap2.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------\n",
    "## 7. Целевые функции\n",
    "\n",
    "Выбор вида целевой функции определяется задачей и соображениями удобства использования. Вообще, общие требования для вида целевой функции:\n",
    "\n",
    "1. Зависимость от выходных активаций сети.\n",
    "2. Дифференцируемость.\n",
    "3. Выражение производной через сумму производных по отдельным примерам.\n",
    "\n",
    "### 7.1. L1, L2 регуляризация\n",
    "\n",
    "Идея — бороться со слишком большими значениями весов.\n",
    "\n",
    "$$\n",
    "J_{L2} = J_0 + J_{reg} = J_0 + \\dfrac{\\lambda}{2 m} \\sum_{l, j, k} \\left(w^l_{jk}\\right)^2 \\,, \\quad\n",
    "\\dfrac{\\partial J_{L2}}{\\partial w^l_{jk}} = \\dfrac{\\partial J_0}{\\partial w^l_{jk}} + \\dfrac{\\lambda}{m} \\, w^l_{jk} \\,,\n",
    "$$\n",
    "\n",
    "$$\n",
    "J_{L1} = J_0 + J_{reg} = J_0 + \\dfrac{\\lambda}{m} \\sum_{l, j, k} \\left|w^l_{jk}\\right| \\,, \\quad\n",
    "\\dfrac{\\partial J_{L1}}{\\partial w^l_{jk}} = \\dfrac{\\partial J_0}{\\partial w^l_{jk}} + \\dfrac{\\lambda}{m} \\, \\mathrm{sign} \\left( w^l_{jk} \\right).\n",
    "$$\n",
    "\n",
    "- Elastic net — комбинация L1 и L2.\n",
    "- $\\lambda$ — гиперпараметр модели, который определяет насколько сильно модель штрафуется за большие веса. \n",
    "- $m$ не обязательно должна входить в уравнения\n",
    "\n",
    "### 7.2. Cross-entropy loss\n",
    "\n",
    "Удобный вид функции потерь для задач классификации, который максимизирует правдоподобность предсказаний (используется вместо квадратичной функции потерь):\n",
    "\n",
    "$$\n",
    "J_{ce} = - \\dfrac{1}{n} \\sum_{i, j} \\left( y^{(i)}_j \\, \\log a^{(i)}_j + \\left( 1 - y^{(i)}_j \\right) \\log \\left( 1 - a^{(i)}_j \\right) \\right)\\,, \\text{где } \\, y^{(i)} = \\begin{pmatrix} y^{(i)}_1 \\\\ \\vdots \\\\ y^{(i)}_m \\end{pmatrix} \\,.\n",
    "$$\n",
    "\n",
    "\n",
    "Здесь $y^{(i)}$ — единичный вектор ответа для данного примера, а $a^{(i)}$ — соответствующий ему вектор предсказаний.\n",
    "\n",
    "\n",
    "|     |  y  |  a  |\n",
    "|:---:|:---:|:---:|\n",
    "|  1  |  1  | 0.7 |\n",
    "|  2  |  0  | 0.4 |\n",
    "| ... | ... | ... |\n",
    "|  n  |  1  | 0.6 |\n",
    "\n",
    "Вероятность получить данный набор данных $P = 0.7 \\cdot 0.6 \\cdot \\ldots \\cdot 0.6$ (для идеальной модели $P = 1$). Тогда **функция правдоподобия** для данного набора данных и предсказаний будет равна \n",
    "\n",
    "$$\n",
    "L\\left( y^{(i)}, a^{(i)} \\right) = \\prod_{i = 1}^n {a^{(i)}}^{y^{(i)}} \\, \\left( 1 - a^{(i)} \\right)^{(1 - y^{(i)})}\\,.\n",
    "$$\n",
    "\n",
    "Если взять логарифм от этого выражения и обратить знак то и получим кросс-энтропийную функцию потерь."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------\n",
    "## 8. Мониторинг состояния сети\n",
    "\n",
    "### 8.1. Проблема переобучения\n",
    "\n",
    "Задача — научить модель давать правильный ответ на новых данных. Проблема переобучения возникает, когда сеть вместо нахождения закономерностей просто выучивает весь набор данных. Например, так получится при выборе слишком большого количества параметров; тогда хоть и на известных данных точность предсказаний будет максимальна, на неизвестных — получатся неверные ответы (см. про **явление волнистости** в численных методах).\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"data/overlearning.png\" width=\"400\" title=\"overlearning\"></p>\n",
    "\n",
    "Чтобы не выбрать неудачную модель следует:  \n",
    "- использовать бритву Окамы.\n",
    "- смотреть результаты работы сети на неизвестных данных.\n",
    "- learning curves, train/test error rate, кросс-валидация и cross-validation error rate, визуализация и т.д.\n",
    "\n",
    "### 8.2. Кросс-валидация\n",
    "\n",
    "Чтобы выбрать среди множества моделей лучшую используют метод **кросс-валидации**. Он заключается в том, что всё множество дынных сначала разбивается на 3 части: **тренировочную**, **валидационную** и **тестовую**. Модели обучаются на тренировочном множестве, затем на валидационном множестве мы их сравниваем, а потом на тестовом проверяем лучшие из них.\n",
    "\n",
    "Часто данные для тренировок разбивают на 5 частей, первую их них считают валидационной, а остальные тренировочными. Затем процесс повторяют, но валидационной считают уже вторую часть данных, затем третью и т.д. (хотя иногда тренируют на всех данных, за исключением одного скользящего примера).\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"data/cross-validation.png\" width=\"560\" title=\"cross-validation\"></p>\n",
    "\n",
    "### 8.3. Визуализация\n",
    "\n",
    "1. Построение **кривых обучения** моделей при обучении на тренировочном множестве (зависимость значения целевой функции от номера эпохи обучения).\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"data/learning_curve.png\" width=\"400\" title=\"learning curve\"></p>\n",
    "\n",
    "2. Построение кривых **точности классификации** (количество правильных ответов к общему числу примеров).\n",
    "\n",
    "3. Построение **тепловых карт** (heatmap) для матриц весов (позволяет визуализировать силу связи между нейронами в соседних слоях).\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"data/heatmap.PNG\" width=\"400\" title=\"heatmap\"></p>\n",
    "\n",
    "4. Построение гистограмм для весов сети.\n",
    "\n",
    "Также можно изучать на какой стимул больше всего реагирует конкретный нейрон скрытного слоя (ищется методом градиентного подъёма) или как по мнению сети, например, выглядят типичные представители классов.\n",
    "\n",
    "- http://yosinski.com/deepvis\n",
    "- https://vimeo.com/132700334"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
